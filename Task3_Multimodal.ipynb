{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0241770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1ced8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"mimic_multimodal_dataset.npz\", allow_pickle=True)\n",
    "embeddings = data[\"embeddings\"]\n",
    "labels = data[\"labels\"]\n",
    "texts = data[\"texts\"]\n",
    "subject_ids = data[\"subject_ids\"]\n",
    "study_ids = data[\"study_ids\"]\n",
    "\n",
    "# Get number of samples\n",
    "N = len(embeddings)\n",
    "indices = np.random.RandomState(seed=42).permutation(N)\n",
    "\n",
    "# Compute split sizes\n",
    "n_train = int(0.8 * N)\n",
    "n_valid = int(0.1 * N)\n",
    "n_test = N - n_train - n_valid  # The rest goes to test\n",
    "\n",
    "# Split indices\n",
    "train_idx = indices[:n_train]\n",
    "valid_idx = indices[n_train:n_train + n_valid]\n",
    "test_idx  = indices[n_train + n_valid:]\n",
    "\n",
    "# Split data based on indices\n",
    "train_set = {\n",
    "    \"embeddings\": embeddings[train_idx],\n",
    "    \"labels\": labels[train_idx],\n",
    "    \"texts\": texts[train_idx],\n",
    "    \"subject_ids\": subject_ids[train_idx],\n",
    "    \"study_ids\": study_ids[train_idx]\n",
    "}\n",
    "\n",
    "valid_set = {\n",
    "    \"embeddings\": embeddings[valid_idx],\n",
    "    \"labels\": labels[valid_idx],\n",
    "    \"texts\": texts[valid_idx],\n",
    "    \"subject_ids\": subject_ids[valid_idx],\n",
    "    \"study_ids\": study_ids[valid_idx]\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    \"embeddings\": embeddings[test_idx],\n",
    "    \"labels\": labels[test_idx],\n",
    "    \"texts\": texts[test_idx],\n",
    "    \"subject_ids\": subject_ids[test_idx],\n",
    "    \"study_ids\": study_ids[test_idx]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df64476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all pathologies list\n",
    "pathologies = [\n",
    "        \"Enlarged Cardiomediastinum\",\n",
    "        \"Cardiomegaly\",\n",
    "        \"Lung Opacity\",\n",
    "        \"Lung Lesion\",\n",
    "        \"Edema\",\n",
    "        \"Consolidation\",\n",
    "        \"Pneumonia\",\n",
    "        \"Atelectasis\",\n",
    "        \"Pneumothorax\",\n",
    "        \"Pleural Effusion\",\n",
    "        \"Pleural Other\",\n",
    "        \"Fracture\",\n",
    "        \"Support Devices\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=128):\n",
    "        self.embeddings = data_dict[\"embeddings\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.texts = data_dict[\"texts\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Tokenize report text using BERT tokenizer\n",
    "        text = str(self.texts[idx])\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": image_embedding,\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # [seq_len]\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # [seq_len]\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8144a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfe2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_set, tokenizer)\n",
    "valid_dataset = MultimodalDataset(valid_set, tokenizer)\n",
    "test_dataset  = MultimodalDataset(test_set, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b5ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (36502, 1376)\n",
      "labels shape: (36502, 13)\n",
      "texts length: 36502\n",
      "subject_ids shape: (36502,)\n",
      "study_ids shape: (36502,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of each component (number of samples & dimensions)\n",
    "print(\"embeddings shape:\", train_set[\"embeddings\"].shape)      # (N, 1376)\n",
    "print(\"labels shape:\", train_set[\"labels\"].shape)              # (N, 13)\n",
    "print(\"texts length:\", len(train_set[\"texts\"]))                # N\n",
    "print(\"subject_ids shape:\", train_set[\"subject_ids\"].shape)    # (N,)\n",
    "print(\"study_ids shape:\", train_set[\"study_ids\"].shape)        # (N,)\n",
    "# [B, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9990d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample index: 10\n",
      "Embedding vector (first 5 dims): [-0.10261969 -0.9120668   0.88922745 -1.6479412  -0.41724488]\n",
      "Label vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Text sample:                                  FINAL REPORT\n",
      " EXAMINATION:  CHEST RADIOGRAPHS\n",
      " \n",
      " INDICATION:  Chest pain.\n",
      " \n",
      " TECHNIQUE:  Chest, PA and lateral.\n",
      " \n",
      " COMPARISON:  None.\n",
      " \n",
      " FINDINGS: \n",
      " \n",
      " The heart is normal in size. There is patchy calcification along the aortic\n",
      " arch. The lungs appear clear. There are no pleural effusions or pneumothorax.\n",
      " \n",
      " IMPRESSION: \n",
      " \n",
      " No evidence of acute cardiopulmonary disease.\n",
      "\n",
      "Subject ID: 19346252\n",
      "Study ID: 51125476\n"
     ]
    }
   ],
   "source": [
    "i = 10 \n",
    "\n",
    "print(\"\\nSample index:\", i)\n",
    "print(\"Embedding vector (first 5 dims):\", train_set[\"embeddings\"][i][:5])\n",
    "print(\"Label vector:\", train_set[\"labels\"][i])\n",
    "print(\"Text sample:\", train_set[\"texts\"][i])\n",
    "print(\"Subject ID:\", train_set[\"subject_ids\"][i])\n",
    "print(\"Study ID:\", train_set[\"study_ids\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6487f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        mask = ~torch.isnan(labels)\n",
    "        labels = torch.where(mask, labels, torch.zeros_like(labels))  # Fill NaN for safety\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = torch.clamp(probs, self.eps, 1 - self.eps)\n",
    "\n",
    "        pos_loss = labels * ((1 - probs) ** self.gamma_pos) * torch.log(probs)\n",
    "        neg_loss = (1 - labels) * (probs ** self.gamma_neg) * torch.log(1 - probs)\n",
    "\n",
    "        loss = - (pos_loss + neg_loss)\n",
    "        loss = loss[mask]  # apply mask\n",
    "\n",
    "        if loss.numel() == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd88841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalFusionModel(nn.Module):\n",
    "    def __init__(self, image_dim=1376, text_dim=768, hidden_dim=512, num_labels=13):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project image and text to shared hidden space\n",
    "        self.img_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        self.txt_proj = nn.Linear(text_dim, hidden_dim)\n",
    "\n",
    "        # Transformer encoder for cross-modal fusion\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.fusion_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Multi-label classification head\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, image_emb, text_emb):\n",
    "        \"\"\"\n",
    "        image_emb: Tensor of shape [B, image_dim]\n",
    "        text_emb: Tensor of shape [B, text_dim] (e.g., BERT CLS token)\n",
    "        \"\"\"\n",
    "        # Project both modalities into common hidden space\n",
    "        img_feat = self.img_proj(image_emb)   # [B, hidden_dim]\n",
    "        txt_feat = self.txt_proj(text_emb)    # [B, hidden_dim]\n",
    "\n",
    "        # Stack: [B, 2, hidden_dim] for transformer input\n",
    "        fused = torch.stack([img_feat, txt_feat], dim=1)\n",
    "\n",
    "        # Transformer-based fusion\n",
    "        fused_out = self.fusion_encoder(fused)  # [B, 2, hidden_dim]\n",
    "\n",
    "        # Mean pooling over modalities\n",
    "        fusion_repr = fused_out.mean(dim=1)     # [B, hidden_dim]\n",
    "\n",
    "        # Output: raw logits (use sigmoid + BCE or ASL externally)\n",
    "        logits = self.classifier(fusion_repr)   # [B, num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8cdde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ClinicalBERT model\n",
    "text_encoder = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)\n",
    "\n",
    "# Initialize the multimodal model\n",
    "model = MultiModalFusionModel().to(device)\n",
    "\n",
    "# Forward pass example\n",
    "for batch in train_loader:\n",
    "    image = batch[\"image\"].to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_feat = text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state[:, 0, :]  # [B, 768]\n",
    "\n",
    "    outputs = model(image, text_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4086acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MaskedAsymmetricLoss(gamma_pos=0.0, gamma_neg=4.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493bb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, text_encoder, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            text_feat = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            logits = model(image, text_feat)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "\n",
    "            # Flatten & mask NaNs for evaluation\n",
    "            mask = ~torch.isnan(label)\n",
    "            all_preds.append(preds[mask].cpu().numpy())\n",
    "            all_targets.append(label[mask].cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_targets)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6bd0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, text_encoder, loss_fn, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    text_encoder.to(device)\n",
    "    text_encoder.eval()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        skipped_batches = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            # Skip batches where all labels are NaN\n",
    "            if torch.isnan(label).all():\n",
    "                print(\"Skipping batch with all NaN labels.\")\n",
    "                skipped_batches += 1\n",
    "                continue\n",
    "\n",
    "            # Extract text features\n",
    "            with torch.no_grad():\n",
    "                text_feat = text_encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                ).last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "            logits = model(image, text_feat)\n",
    "            loss = loss_fn(logits, label)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Skipping batch due to NaN loss.\")\n",
    "                skipped_batches += 1\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / (len(train_loader) - skipped_batches + 1e-6)\n",
    "        print(f\"\\n Epoch {epoch+1} completed. Avg Loss = {avg_loss:.4f} | Skipped Batches: {skipped_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9434eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 1141/1141 [25:23<00:00,  1.34s/it, loss=0.0956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 completed. Avg Loss = 0.0490 | Skipped Batches: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    text_encoder=text_encoder,\n",
    "    loss_fn=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96c808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multimodal_fusion_epoch1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1043518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, text_encoder, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            text_feat = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            logits = model(image, text_feat)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_preds.append(probs.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [N, num_labels]\n",
    "    all_labels = torch.cat(all_labels, dim=0)  # [N, num_labels]\n",
    "\n",
    "    num_labels = all_labels.shape[1]\n",
    "    f1_per_label = []\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        y_true = all_labels[:, i]\n",
    "        y_pred = all_preds[:, i]\n",
    "\n",
    "        mask = ~torch.isnan(y_true)\n",
    "        if mask.sum() == 0:\n",
    "            continue  # Skip this label (completely missing)\n",
    "\n",
    "        y_true = y_true[mask].numpy()\n",
    "        y_pred = (y_pred[mask] > 0.5).float().numpy()\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        f1_per_label.append(f1)\n",
    "\n",
    "    macro_f1 = np.mean(f1_per_label)\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "946bb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_label(model, dataloader, text_encoder, device, label_names=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            text_feat = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            logits = model(image, text_feat)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_preds.append(probs.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [N, num_labels]\n",
    "    all_labels = torch.cat(all_labels, dim=0)  # [N, num_labels]\n",
    "\n",
    "    num_labels = all_labels.shape[1]\n",
    "    f1_results = []\n",
    "    ap_results = []\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        y_true = all_labels[:, i]\n",
    "        y_score = all_preds[:, i]\n",
    "\n",
    "        mask = ~torch.isnan(y_true)\n",
    "        if mask.sum() == 0:\n",
    "            f1_results.append(None)\n",
    "            ap_results.append(None)\n",
    "            continue\n",
    "\n",
    "        y_true = y_true[mask].numpy()\n",
    "        y_pred = (y_score[mask] > 0.5).float().numpy()\n",
    "        y_score = y_score[mask].numpy()\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "        f1_results.append(f1)\n",
    "        ap_results.append(ap)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n Per-label evaluation:\")\n",
    "    for i in range(num_labels):\n",
    "        name = label_names[i] if label_names else f\"Label {i}\"\n",
    "        if f1_results[i] is not None:\n",
    "            print(f\"{name:<25} | F1: {f1_results[i]:.3f} | AP: {ap_results[i]:.3f}\")\n",
    "        else:\n",
    "            print(f\"{name:<25} | F1:   N/A | AP:   N/A\")\n",
    "\n",
    "    macro_f1 = np.nanmean([f for f in f1_results if f is not None])\n",
    "    print(f\"\\n Macro F1 (valid labels): {macro_f1:.4f}\")\n",
    "    return f1_results, ap_results, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209c94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Per-label evaluation:\n",
      "Enlarged Cardiomediastinum | F1: 0.598 | AP: 0.597\n",
      "Cardiomegaly              | F1: 0.546 | AP: 0.629\n",
      "Lung Opacity              | F1: 0.345 | AP: 0.350\n",
      "Lung Lesion               | F1: 0.517 | AP: 0.509\n",
      "Edema                     | F1: 0.215 | AP: 0.160\n",
      "Consolidation             | F1: 0.248 | AP: 0.409\n",
      "Pneumonia                 | F1: 0.402 | AP: 0.391\n",
      "Atelectasis               | F1: 0.597 | AP: 0.639\n",
      "Pneumothorax              | F1: 0.637 | AP: 0.712\n",
      "Pleural Effusion          | F1: 0.200 | AP: 0.222\n",
      "Pleural Other             | F1: 0.348 | AP: 0.433\n",
      "Fracture                  | F1: 0.262 | AP: 0.246\n",
      "Support Devices           | F1: 0.419 | AP: 0.512\n",
      "\n",
      " Macro F1 (valid labels): 0.4103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5978152929493545,\n",
       "  0.5456012913640033,\n",
       "  0.34509803921568627,\n",
       "  0.5171102661596958,\n",
       "  0.21524663677130046,\n",
       "  0.24825174825174826,\n",
       "  0.40162271805273836,\n",
       "  0.5966850828729282,\n",
       "  0.6368593238822247,\n",
       "  0.2,\n",
       "  0.3483043079743355,\n",
       "  0.2619047619047619,\n",
       "  0.4189723320158103],\n",
       " [np.float64(0.5969954061204436),\n",
       "  np.float64(0.6292393412009127),\n",
       "  np.float64(0.3498741646655957),\n",
       "  np.float64(0.5094693797951755),\n",
       "  np.float64(0.15974951584784547),\n",
       "  np.float64(0.4089238899782475),\n",
       "  np.float64(0.39082492343833475),\n",
       "  np.float64(0.6394214503969154),\n",
       "  np.float64(0.71237870322538),\n",
       "  np.float64(0.22241993384563286),\n",
       "  np.float64(0.43265489241697885),\n",
       "  np.float64(0.2464231249109456),\n",
       "  np.float64(0.51173342293466)],\n",
       " np.float64(0.410267061647276))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\",\n",
    "    \"Lung Lesion\", \"Edema\", \"Consolidation\", \"Pneumonia\", \"Atelectasis\",\n",
    "    \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\", \"Fracture\", \"Support Devices\"\n",
    "]\n",
    "\n",
    "evaluate_per_label(model, test_loader, text_encoder, device, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49b0aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, average_precision_score, f1_score\n",
    "\n",
    "def safe_get(report, class_label, metric):\n",
    "    if isinstance(class_label, (int, float)):\n",
    "        keys_to_try = [class_label, str(class_label), f\"{float(class_label):.1f}\"]\n",
    "    else:\n",
    "        keys_to_try = [class_label]\n",
    "\n",
    "    for key in keys_to_try:\n",
    "        if key in report and metric in report[key]:\n",
    "            return report[key][metric]\n",
    "    return np.nan\n",
    "\n",
    "def evaluate_detailed_per_label(model, dataloader, text_encoder, device, label_names=None, save_path=None):\n",
    "    model.eval()\n",
    "    text_encoder.eval()\n",
    "    model.to(device)\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            text_feat = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            logits = model(image, text_feat)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_preds.append(probs.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [N, num_labels]\n",
    "    all_labels = torch.cat(all_labels, dim=0)  # [N, num_labels]\n",
    "\n",
    "    num_labels = all_labels.shape[1]\n",
    "    report_dict = {}\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        y_true = all_labels[:, i]\n",
    "        y_score = all_preds[:, i]\n",
    "\n",
    "        mask = ~torch.isnan(y_true)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = y_true[mask].numpy()\n",
    "        y_pred = (y_score[mask] > 0.5).float().numpy()\n",
    "        y_score = y_score[mask].numpy()\n",
    "\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "        acc = (y_true == y_pred).mean()\n",
    "        ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "        label_name = label_names[i] if label_names else f\"Label {i}\"\n",
    "        report_dict[label_name] = {\n",
    "            # Class 0\n",
    "            'precision_0': safe_get(report, 0, 'precision'),\n",
    "            'recall_0': safe_get(report, 0, 'recall'),\n",
    "            'f1-score_0': safe_get(report, 0, 'f1-score'),\n",
    "\n",
    "            # Class 1\n",
    "            'precision_1': safe_get(report, 1, 'precision'),\n",
    "            'recall_1': safe_get(report, 1, 'recall'),\n",
    "            'f1-score_1': safe_get(report, 1, 'f1-score'),\n",
    "\n",
    "            # Macro avg\n",
    "            'precision': safe_get(report, 'macro avg', 'precision'),\n",
    "            'recall': safe_get(report, 'macro avg', 'recall'),\n",
    "            'f1-score': safe_get(report, 'macro avg', 'f1-score'),\n",
    "            'support': safe_get(report, 'macro avg', 'support'),\n",
    "\n",
    "            # AP and Accuracy\n",
    "            'AP': ap,\n",
    "            'accuracy': acc\n",
    "        }\n",
    "\n",
    "    # Save as Transposed DataFrame (horizontal table)\n",
    "    df = pd.DataFrame(report_dict).T.round(4)\n",
    "    df = df.transpose()\n",
    "    if save_path:\n",
    "        df.to_csv(save_path)\n",
    "        print(f\"\\n Saved detailed metrics to (horizontal): {save_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "705d1dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saved detailed metrics to (horizontal): detailed_metrics_per_label.csv\n"
     ]
    }
   ],
   "source": [
    "df = evaluate_detailed_per_label(\n",
    "    model, test_loader, text_encoder, device,\n",
    "    label_names=label_names,\n",
    "    save_path=\"detailed_metrics_per_label.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
